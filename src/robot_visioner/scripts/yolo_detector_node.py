#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import Point, Quaternion
from visualization_msgs.msg import Marker, MarkerArray
from std_msgs.msg import Header, ColorRGBA, String
from cv_bridge import CvBridge
import cv2
import numpy as np
import time
import math
import json
from scipy.spatial.transform import Rotation

class YOLODetectorNode(Node):
    """å¢å¼ºç‰ˆYOLOæ£€æµ‹èŠ‚ç‚¹ - æ”¯æŒç‰©ä½“åç§°è¯†åˆ«å’Œæ—‹è½¬è§’åº¦"""
    
    def __init__(self):
        super().__init__('yolo_detector')
        
        # åˆå§‹åŒ–æ—¥å¿—æ—¶é—´æˆ³
        self._last_log_time = 0
        self._last_info_time = 0
        
        # é€ä¸ªå£°æ˜å‚æ•°
        self.declare_parameter('model_path', 'yolov8n-seg.pt')
        self.declare_parameter('confidence_threshold', 0.5)
        self.declare_parameter('iou_threshold', 0.4)
        self.declare_parameter('device', 'cpu')
        
        # è¾“å…¥è¾“å‡ºè¯é¢˜
        self.declare_parameter('input_topic', '/camera/color/image_raw')
        self.declare_parameter('camera_info_topic', '/camera/color/camera_info')
        self.declare_parameter('output_detection_topic', '/yolo/detection_result')
        self.declare_parameter('output_mask_topic', '/yolo/mask')
        self.declare_parameter('output_combined_topic', '/yolo/combined_result')
        
        # åˆ†å‰²ç›¸å…³å‚æ•°
        self.declare_parameter('enable_segmentation', True)
        self.declare_parameter('mask_threshold', 0.5)
        self.declare_parameter('target_classes', '')
        self.declare_parameter('publish_individual_masks', False)
        
        # å›¾åƒå‚æ•°
        self.declare_parameter('image_width', 640)
        self.declare_parameter('image_height', 480)
        
        # æ€§èƒ½å‚æ•°
        self.declare_parameter('max_detections', 100)
        self.declare_parameter('agnostic_nms', False)
        
        # RVizæ ‡è®°å‚æ•°
        self.declare_parameter('output_marker_topic', '/yolo/markers')
        self.declare_parameter('frame_id', 'camera_color_optical_frame')
        self.declare_parameter('enable_rviz_markers', True)
        self.declare_parameter('marker_scale', 0.1)
        self.declare_parameter('show_rotation_text', True)
        self.declare_parameter('default_object_depth', 1.0)
        
        # è·å–å‚æ•°å€¼
        self.load_parameters()
        
        # åˆå§‹åŒ–cv_bridge
        self.bridge = CvBridge()
        
        # YOLOç±»åˆ«IDåˆ°ç‰©ä½“åç§°çš„æ˜ å°„
        self.class_id_to_object_name = {
            0: "luosi",
            1: "pen",
            2: "xiguan",
            3: "zhoucheng",
        }
        
        # ç›®æ ‡ç‰©ä½“è¿‡æ»¤ï¼ˆåªå¤„ç†è¿™äº›ç‰©ä½“ç”¨äºæŠ“å–ä»»åŠ¡ï¼‰
        self.target_objects = ["luosi", "pen", "xiguan", "zhoucheng"]
        
        # å°è¯•åŠ è½½YOLOæ¨¡å‹
        self.load_model()
        
        # åˆ›å»ºè®¢é˜…è€…å’Œå‘å¸ƒè€…
        self.setup_topics()
        
        # ç›¸æœºä¿¡æ¯
        self.camera_info = None
        
        # é¢œè‰²æ˜ å°„ç”¨äºå¯è§†åŒ–
        self.colors = self.generate_colors(80)
        
        self.log_initialization_info()
    
    def throttled_log_info(self, message, throttle_sec=2.0):
        """èŠ‚æµæ—¥å¿—ä¿¡æ¯"""
        current_time = time.time()
        if current_time - self._last_info_time > throttle_sec:
            self.get_logger().info(message)
            self._last_info_time = current_time
    
    def load_parameters(self):
        """è·å–å‚æ•°å€¼"""
        self.model_path = self.get_parameter('model_path').value
        self.confidence_threshold = self.get_parameter('confidence_threshold').value
        self.iou_threshold = self.get_parameter('iou_threshold').value
        self.device = self.get_parameter('device').value
        
        self.input_topic = self.get_parameter('input_topic').value
        self.camera_info_topic = self.get_parameter('camera_info_topic').value
        self.output_detection_topic = self.get_parameter('output_detection_topic').value
        self.output_mask_topic = self.get_parameter('output_mask_topic').value
        self.output_combined_topic = self.get_parameter('output_combined_topic').value
        
        self.enable_segmentation = self.get_parameter('enable_segmentation').value
        self.mask_threshold = self.get_parameter('mask_threshold').value
        self.publish_individual_masks = self.get_parameter('publish_individual_masks').value
        
        # è§£æç›®æ ‡ç±»åˆ«
        target_classes_str = self.get_parameter('target_classes').value
        if target_classes_str:
            try:
                self.target_classes = [int(x.strip()) for x in target_classes_str.split(',')]
            except:
                self.target_classes = []
        else:
            self.target_classes = []
        
        self.image_width = self.get_parameter('image_width').value
        self.image_height = self.get_parameter('image_height').value
        self.max_detections = self.get_parameter('max_detections').value
        self.agnostic_nms = self.get_parameter('agnostic_nms').value
        
        # RVizå‚æ•°
        self.output_marker_topic = self.get_parameter('output_marker_topic').value
        self.frame_id = self.get_parameter('frame_id').value
        self.enable_rviz_markers = self.get_parameter('enable_rviz_markers').value
        self.marker_scale = self.get_parameter('marker_scale').value
        self.show_rotation_text = self.get_parameter('show_rotation_text').value
        self.default_object_depth = self.get_parameter('default_object_depth').value
    
    def load_model(self):
        """åŠ è½½YOLOæ¨¡å‹"""
        try:
            from ultralytics import YOLO
            
            self.get_logger().info(f'ğŸ¤– Loading YOLO model: {self.model_path}')
            self.model = YOLO(self.model_path)
            
            # æ£€æŸ¥è®¾å¤‡
            if self.device == 'cuda':
                try:
                    import torch
                    if torch.cuda.is_available():
                        self.model.to('cuda')
                        self.get_logger().info('âœ… Using CUDA device')
                    else:
                        self.get_logger().warn('âš ï¸ CUDA not available, using CPU')
                        self.device = 'cpu'
                except:
                    self.get_logger().warn('âš ï¸ PyTorch not available, using CPU')
                    self.device = 'cpu'
            
            self.get_logger().info(f'âœ… Model loaded successfully on device: {self.device}')
            self.get_logger().info(f'ğŸ“‹ Model classes: {self.model.names}')
            self.model_loaded = True
            
        except ImportError:
            self.get_logger().error('âŒ ultralytics not installed. Using simple fallback.')
            self.model_loaded = False
        except Exception as e:
            self.get_logger().error(f'âŒ Failed to load YOLO model: {str(e)}. Using fallback.')
            self.model_loaded = False
    
    def setup_topics(self):
        """è®¾ç½®è®¢é˜…è€…å’Œå‘å¸ƒè€…"""
        # è®¢é˜…è€…
        self.image_sub = self.create_subscription(
            Image, self.input_topic, self.image_callback, 10)
            
        self.camera_info_sub = self.create_subscription(
            CameraInfo, self.camera_info_topic, self.camera_info_callback, 10)
        
        # å‘å¸ƒè€…
        self.detection_result_pub = self.create_publisher(
            Image, self.output_detection_topic, 10)
            
        if self.enable_segmentation:
            self.mask_pub = self.create_publisher(
                Image, self.output_mask_topic, 10)
                
            self.combined_result_pub = self.create_publisher(
                Image, self.output_combined_topic, 10)
        
        # RVizæ ‡è®°å‘å¸ƒè€…
        if self.enable_rviz_markers:
            self.marker_pub = self.create_publisher(
                MarkerArray, self.output_marker_topic, 10)
        
        # æ–°å¢ï¼šæ£€æµ‹ä¿¡æ¯å‘å¸ƒè€…ï¼ˆç»™robot_visionerï¼‰
        self.detection_info_pub = self.create_publisher(
            String, '/yolo/detection_info', 10)
        
        self.get_logger().info(f'ğŸ“¡ Subscribed to: {self.input_topic}')
        self.get_logger().info(f'ğŸ“¤ Publishing detection to: {self.output_detection_topic}')
        if self.enable_segmentation:
            self.get_logger().info(f'ğŸ“¤ Publishing mask to: {self.output_mask_topic}')
        if self.enable_rviz_markers:
            self.get_logger().info(f'ğŸ“¤ Publishing markers to: {self.output_marker_topic}')
        self.get_logger().info(f'ğŸ“¤ Publishing detection info to: /yolo/detection_info')
    
    def generate_colors(self, num_classes):
        """ç”Ÿæˆç”¨äºå¯è§†åŒ–çš„éšæœºé¢œè‰²"""
        colors = []
        np.random.seed(42)
        for _ in range(num_classes):
            color = tuple(np.random.randint(50, 255, 3).tolist())
            colors.append(color)
        return colors
    
    def camera_info_callback(self, msg):
        """ç›¸æœºä¿¡æ¯å›è°ƒå‡½æ•°"""
        self.camera_info = msg
    
    def calculate_rotation_angle(self, mask):
        """è®¡ç®—ç‰©ä½“çš„æ—‹è½¬è§’åº¦"""
        try:
            # æŸ¥æ‰¾è½®å»“
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if not contours:
                return 0.0, None
            
            # æ‰¾åˆ°æœ€å¤§è½®å»“
            largest_contour = max(contours, key=cv2.contourArea)
            
            # ä½¿ç”¨æœ€å°å¤–æ¥çŸ©å½¢è®¡ç®—æ—‹è½¬è§’åº¦
            rect = cv2.minAreaRect(largest_contour)
            center, (width, height), angle = rect
            
            # OpenCVçš„è§’åº¦èŒƒå›´æ˜¯[-90, 0]ï¼Œè½¬æ¢ä¸º[0, 180]
            if width < height:
                angle = angle + 90
            
            # æ ‡å‡†åŒ–è§’åº¦åˆ°[0, 360]èŒƒå›´
            angle = angle % 360
            
            return angle, center
            
        except Exception as e:
            self.get_logger().debug(f'è§’åº¦è®¡ç®—å¤±è´¥: {str(e)}')
            return 0.0, None
    
    def pixel_to_world(self, pixel_x, pixel_y, depth=None):
        """å°†åƒç´ åæ ‡è½¬æ¢ä¸ºä¸–ç•Œåæ ‡"""
        if self.camera_info is None:
            return None
        
        if depth is None:
            depth = self.default_object_depth
        
        # ä»ç›¸æœºå†…å‚çŸ©é˜µè·å–ç„¦è·å’Œä¸»ç‚¹
        fx = self.camera_info.k[0]
        fy = self.camera_info.k[4]
        cx = self.camera_info.k[2]
        cy = self.camera_info.k[5]
        
        # è½¬æ¢ä¸ºç›¸æœºåæ ‡ç³»
        x = (pixel_x - cx) * depth / fx
        y = (pixel_y - cy) * depth / fy
        z = depth
        
        return Point(x=x, y=y, z=z)
    
    def publish_detection_info(self, detections_data, timestamp):
        """å‘å¸ƒæ£€æµ‹ä¿¡æ¯ç»™robot_visioner"""
        if not detections_data:
            return
        
        # æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„ç›®æ ‡ç‰©ä½“
        best_detection = None
        best_score = 0
        
        for bbox, class_id, confidence, angle, center in detections_data:
            object_name = self.class_id_to_object_name.get(class_id, "unknown")
            
            # åªå¤„ç†ç›®æ ‡ç‰©ä½“
            if object_name in self.target_objects and confidence > best_score:
                best_detection = {
                    'object_name': object_name,
                    'center_x': float(center[0]),
                    'center_y': float(center[1]), 
                    'rotation_angle': float(angle),
                    'confidence': float(confidence),
                    'class_id': int(class_id),
                    'bbox': [float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])]
                }
                best_score = confidence
        
        if best_detection:
            # å‘å¸ƒæ£€æµ‹ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰
            detection_msg = String()
            detection_msg.data = json.dumps(best_detection)
            self.detection_info_pub.publish(detection_msg)
            
            self.get_logger().debug(f"å‘å¸ƒæ£€æµ‹ä¿¡æ¯: {best_detection['object_name']} "
                                  f"è§’åº¦: {best_detection['rotation_angle']:.1f}Â°")
    
    def process_detection_results(self, image, results, detections_for_markers):
        """å¤„ç†æ£€æµ‹ç»“æœå¹¶ç»˜åˆ¶"""
        annotated_image = image.copy()
        
        if not hasattr(results, 'boxes') or results.boxes is None:
            return annotated_image
        
        boxes = results.boxes.xyxy.cpu().numpy()
        scores = results.boxes.conf.cpu().numpy()
        classes = results.boxes.cls.cpu().numpy().astype(int)
        
        # æ£€æŸ¥æ•°ç»„æ˜¯å¦ä¸ºç©º
        if len(boxes) == 0 or len(scores) == 0 or len(classes) == 0:
            self.get_logger().debug('âš ï¸ æœªå‘ç°æ£€æµ‹ç»“æœï¼Œè¿”å›åŸå§‹å›¾åƒ')
            return annotated_image
        
        self.get_logger().debug(f'ğŸ¯ åŸå§‹æ£€æµ‹: {len(boxes)} ä¸ªè¾¹ç•Œæ¡†')
        
        # å®‰å…¨åœ°è®¡ç®—min/maxï¼Œé¿å…ç©ºæ•°ç»„é”™è¯¯
        if len(scores) > 0:
            self.get_logger().debug(f'å¾—åˆ†èŒƒå›´: {scores.min():.3f}-{scores.max():.3f}')
        
        # è¿‡æ»¤ç›®æ ‡ç±»åˆ«æ—¶æ·»åŠ æ£€æŸ¥
        if self.target_classes:
            valid_indices = [i for i, cls in enumerate(classes) if cls in self.target_classes]
            if len(valid_indices) == 0:
                self.throttled_log_info('âš ï¸ æ²¡æœ‰ç‰©ä½“åŒ¹é…ç›®æ ‡ç±»åˆ«')
                return annotated_image
                
            boxes = boxes[valid_indices]
            scores = scores[valid_indices]
            classes = classes[valid_indices]
            self.throttled_log_info(f'ğŸ¯ After class filtering: {len(boxes)} boxes')
        
        # è·å–åˆ†å‰²maskç”¨äºè§’åº¦è®¡ç®—
        masks = getattr(results, 'masks', None)
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        for i, (box, score, cls) in enumerate(zip(boxes, scores, classes)):
            x1, y1, x2, y2 = map(int, box)
            
            # è·å–ç‰©ä½“åç§°
            object_name = self.class_id_to_object_name.get(cls, f'Class_{cls}')
            
            # é€‰æ‹©é¢œè‰²
            color = self.colors[cls % len(self.colors)]
            
            # è®¡ç®—æ—‹è½¬è§’åº¦
            angle = 0.0
            center = ((x1 + x2) / 2, (y1 + y2) / 2)
            
            if masks is not None and i < len(masks.data):
                # ä½¿ç”¨åˆ†å‰²æ©ç è®¡ç®—è§’åº¦
                mask = masks.data[i].cpu().numpy()
                mask_resized = cv2.resize(
                    mask, (image.shape[1], image.shape[0]))
                mask_binary = (mask_resized > self.mask_threshold).astype(np.uint8) * 255
                
                angle, mask_center = self.calculate_rotation_angle(mask_binary)
                if mask_center:
                    center = mask_center
            else:
                # ä½¿ç”¨è¾¹ç•Œæ¡†ä¼°è®¡è§’åº¦
                roi = image[int(y1):int(y2), int(x1):int(x2)]
                if roi.size > 0:
                    gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
                    _, binary_roi = cv2.threshold(gray_roi, 0, 255, 
                                                cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                    
                    angle, roi_center = self.calculate_rotation_angle(binary_roi)
                    if roi_center:
                        center = (x1 + roi_center[0], y1 + roi_center[1])
            
            # å­˜å‚¨æ£€æµ‹ç»“æœç”¨äºRVizæ ‡è®°å’Œä¿¡æ¯å‘å¸ƒ
            bbox = [x1, y1, x2, y2]
            detections_for_markers.append((bbox, cls, score, angle, center))
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)
            
            # ç»˜åˆ¶æ—‹è½¬è§’åº¦æŒ‡ç¤ºçº¿
            center_int = (int(center[0]), int(center[1]))
            angle_rad = math.radians(angle)
            line_length = 30
            end_x = int(center[0] + line_length * math.cos(angle_rad))
            end_y = int(center[1] + line_length * math.sin(angle_rad))
            cv2.arrowedLine(annotated_image, center_int, (end_x, end_y), 
                          (0, 0, 255), 3, tipLength=0.3)
            
            # ç»˜åˆ¶æ ‡ç­¾ - æ˜¾ç¤ºç‰©ä½“åç§°
            label = f'{object_name}: {score:.2f} {angle:.1f}Â°'
            self.draw_label(annotated_image, label, (x1, y1), color)
        
        return annotated_image
    
    def process_segmentation_results(self, image, results):
        """å¤„ç†åˆ†å‰²ç»“æœ"""
        height, width = image.shape[:2]
        combined_mask = np.zeros((height, width), dtype=np.uint8)
        annotated_image = image.copy()
        
        if not hasattr(results, 'masks') or results.masks is None:
            self.throttled_log_info('âš ï¸ No segmentation masks found')
            return None, annotated_image
        
        masks = results.masks.data.cpu().numpy()
        if len(masks) == 0:
            self.throttled_log_info('âš ï¸ æ©ç æ•°ç»„ä¸ºç©º')
            return None, annotated_image
        boxes = results.boxes.xyxy.cpu().numpy() if results.boxes is not None else None
        scores = results.boxes.conf.cpu().numpy() if results.boxes is not None else None
        classes = results.boxes.cls.cpu().numpy().astype(int) if results.boxes is not None else None
        
        if classes is None:
            self.throttled_log_info('âš ï¸ No class information for masks')
            return None, annotated_image
        
        self.get_logger().debug(f'Processing {len(masks)} masks')
        
        valid_mask_count = 0
        
        for i, mask in enumerate(masks):
            if i >= len(classes):
                continue
                
            cls = classes[i]
            score = scores[i] if scores is not None else 1.0
            
            # è¿‡æ»¤ç›®æ ‡ç±»åˆ«
            if self.target_classes and cls not in self.target_classes:
                continue
            
            # è·å–ç‰©ä½“åç§°
            object_name = self.class_id_to_object_name.get(cls, f'Class_{cls}')
            
            # è°ƒæ•´maskå°ºå¯¸
            mask_resized = cv2.resize(mask, (width, height))
            mask_binary = (mask_resized > self.mask_threshold).astype(np.uint8) * 255
            
            # æ£€æŸ¥maskæ˜¯å¦æœ‰æ•ˆ
            mask_pixels = np.sum(mask_binary > 0)
            if mask_pixels == 0:
                continue
            
            # åˆå¹¶åˆ°æ€»maskä¸­
            combined_mask = cv2.bitwise_or(combined_mask, mask_binary)
            valid_mask_count += 1
            
            # åœ¨å¯è§†åŒ–å›¾åƒä¸Šç»˜åˆ¶mask
            color = self.colors[cls % len(self.colors)]
            colored_mask = np.zeros_like(image)
            colored_mask[mask_binary > 0] = color
            
            # å åŠ mask
            annotated_image = cv2.addWeighted(annotated_image, 0.7, colored_mask, 0.3, 0)
        
        if valid_mask_count > 0:
            total_mask_pixels = np.sum(combined_mask > 0)
            self.get_logger().debug(f'âœ… Generated final mask: {valid_mask_count} objects, {total_mask_pixels} pixels')
            return combined_mask, annotated_image
        else:
            self.throttled_log_info('âš ï¸ No valid masks generated')
            return None, annotated_image
    
    def create_detection_markers(self, detections, timestamp):
        """åˆ›å»ºRVizæ ‡è®°"""
        marker_array = MarkerArray()
        
        for i, detection in enumerate(detections):
            bbox, class_id, confidence, angle, center = detection
            
            # è·å–ç‰©ä½“åç§°
            object_name = self.class_id_to_object_name.get(class_id, f'class_{class_id}')
            
            # åˆ›å»ºè¾¹ç•Œæ¡†æ ‡è®°
            bbox_marker = Marker()
            bbox_marker.header.frame_id = self.frame_id
            bbox_marker.header.stamp = timestamp
            bbox_marker.ns = "detection_boxes"
            bbox_marker.id = i * 2
            bbox_marker.type = Marker.CUBE
            bbox_marker.action = Marker.ADD
            
            # è®¡ç®—3Dä½ç½®
            center_3d = self.pixel_to_world(center[0], center[1])
            if center_3d:
                bbox_marker.pose.position = center_3d
            else:
                bbox_marker.pose.position = Point(x=0.0, y=0.0, z=1.0)
            
            # è®¾ç½®æ—‹è½¬ï¼ˆç»•Zè½´æ—‹è½¬ï¼‰
            angle_rad = math.radians(angle)
            r = Rotation.from_euler('z', angle_rad)
            quat = r.as_quat()  # [x, y, z, w]
            bbox_marker.pose.orientation = Quaternion(x=quat[0], y=quat[1], z=quat[2], w=quat[3])
            
            # è®¾ç½®å°ºå¯¸
            bbox_width = (bbox[2] - bbox[0]) * self.marker_scale * 0.001
            bbox_height = (bbox[3] - bbox[1]) * self.marker_scale * 0.001
            bbox_marker.scale.x = max(bbox_width, 0.05)
            bbox_marker.scale.y = max(bbox_height, 0.05)
            bbox_marker.scale.z = 0.02
            
            # è®¾ç½®é¢œè‰²
            color = self.colors[class_id % len(self.colors)]
            bbox_marker.color = ColorRGBA(
                r=color[2]/255.0, g=color[1]/255.0, b=color[0]/255.0, a=0.7)
            
            bbox_marker.lifetime.sec = 1
            marker_array.markers.append(bbox_marker)
            
            # åˆ›å»ºæ–‡æœ¬æ ‡è®°
            if self.show_rotation_text:
                text_marker = Marker()
                text_marker.header.frame_id = self.frame_id
                text_marker.header.stamp = timestamp
                text_marker.ns = "detection_text"
                text_marker.id = i * 2 + 1
                text_marker.type = Marker.TEXT_VIEW_FACING
                text_marker.action = Marker.ADD
                
                if center_3d:
                    text_marker.pose.position = Point(
                        x=center_3d.x, y=center_3d.y - 0.1, z=center_3d.z + 0.05)
                else:
                    text_marker.pose.position = Point(x=0.0, y=-0.1, z=1.05)
                
                text_marker.scale.z = 0.05
                text_marker.color = ColorRGBA(r=1.0, g=1.0, b=1.0, a=1.0)
                
                text_marker.text = f'{object_name}\n{confidence:.2f}\n{angle:.1f}Â°'
                text_marker.lifetime.sec = 1
                
                marker_array.markers.append(text_marker)
        
        return marker_array
    
    def fallback_detection(self, image):
        """ç®€å•çš„fallbackæ£€æµ‹"""
        detection_image = image.copy()
        
        # åœ¨å›¾åƒä¸Šç»˜åˆ¶"No Model"æ–‡æœ¬
        cv2.putText(detection_image, "YOLO Model Not Available", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        
        return detection_image, None, detection_image
    
    def image_callback(self, msg):
        """å›¾åƒæ¶ˆæ¯å›è°ƒå‡½æ•°"""
        try:
            # éªŒè¯è¾“å…¥å›¾åƒ
            if msg.width == 0 or msg.height == 0:
                self.get_logger().error('ğŸš« æ¥æ”¶åˆ°å°ºå¯¸ä¸ºé›¶çš„ç©ºå›¾åƒ')
                return
                
            self.get_logger().debug(f' æ¥æ”¶å›¾åƒ: {msg.width}x{msg.height}')
            
            # å°†ROSå›¾åƒæ¶ˆæ¯è½¬æ¢ä¸ºOpenCVæ ¼å¼
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            
            # éªŒè¯è½¬æ¢åçš„å›¾åƒ
            if cv_image.size == 0:
                self.get_logger().error('ğŸš« è½¬æ¢åçš„å›¾åƒä¸ºç©º')
                return
            
            # å­˜å‚¨æ£€æµ‹ç»“æœç”¨äºRVizæ ‡è®°
            detections_for_markers = []
            
            if self.model_loaded:
                # ä½¿ç”¨YOLOæ¨¡å‹
                results = self.model(
                    cv_image,
                    conf=self.confidence_threshold,
                    iou=self.iou_threshold,
                    max_det=self.max_detections,
                    agnostic_nms=self.agnostic_nms,
                    verbose=False
                )
                
                detection_image = self.process_detection_results(cv_image, results[0], detections_for_markers)
                
                # å‘å¸ƒæ£€æµ‹ä¿¡æ¯ç»™robot_visioner
                if detections_for_markers:
                    self.publish_detection_info(detections_for_markers, msg.header.stamp)
                
                if self.enable_segmentation:
                    combined_mask, combined_image = self.process_segmentation_results(cv_image, results[0])
                else:
                    combined_mask, combined_image = None, detection_image
            else:
                # ä½¿ç”¨ç®€å•çš„fallbackå¤„ç†
                detection_image, combined_mask, combined_image = self.fallback_detection(cv_image)
            
            # å‘å¸ƒç»“æœ
            self.publish_results(detection_image, combined_mask, combined_image, msg.header.stamp)
            
            # å‘å¸ƒRVizæ ‡è®°
            if self.enable_rviz_markers and detections_for_markers:
                marker_array = self.create_detection_markers(detections_for_markers, msg.header.stamp)
                self.marker_pub.publish(marker_array)
                
        except Exception as e:
            self.get_logger().error(f'å›¾åƒå¤„ç†é”™è¯¯: {str(e)}')
            import traceback
            self.get_logger().error(f'å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}')
    
    def publish_results(self, detection_image, combined_mask, combined_image, timestamp):
        """å‘å¸ƒå¤„ç†ç»“æœ"""
        try:
            # å‘å¸ƒæ£€æµ‹ç»“æœå›¾åƒ
            detection_msg = self.bridge.cv2_to_imgmsg(detection_image, "bgr8")
            detection_msg.header.stamp = timestamp
            self.detection_result_pub.publish(detection_msg)
            
            # å‘å¸ƒåˆ†å‰²ç»“æœ
            if self.enable_segmentation and combined_mask is not None:
                mask_msg = self.bridge.cv2_to_imgmsg(combined_mask, "mono8")
                mask_msg.header.stamp = timestamp
                self.mask_pub.publish(mask_msg)
                
                if combined_image is not None:
                    combined_msg = self.bridge.cv2_to_imgmsg(combined_image, "bgr8")
                    combined_msg.header.stamp = timestamp
                    self.combined_result_pub.publish(combined_msg)
                    
        except Exception as e:
            self.get_logger().error(f'å‘å¸ƒç»“æœå¤±è´¥: {str(e)}')
    
    def draw_label(self, image, label, position, color):
        """ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬"""
        x, y = position
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 2
        
        # è·å–æ–‡æœ¬å°ºå¯¸
        (text_width, text_height), _ = cv2.getTextSize(label, font, font_scale, thickness)
        
        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
        cv2.rectangle(image, (x, y - text_height - 10), (x + text_width, y), color, -1)
        
        # ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬
        cv2.putText(image, label, (x, y - 5), font, font_scale, (255, 255, 255), thickness)
    
    def log_initialization_info(self):
        """è®°å½•åˆå§‹åŒ–ä¿¡æ¯"""
        self.get_logger().info('=== YOLO Detector Node with Object Names ===')
        self.get_logger().info(f'ğŸ“¦ Model: {self.model_path}')
        self.get_logger().info(f'ğŸ”§ Device: {self.device}')
        self.get_logger().info(f'âœ… Model loaded: {self.model_loaded}')
        self.get_logger().info(f'ğŸ­ Segmentation: {self.enable_segmentation}')
        self.get_logger().info(f'ğŸ¯ Confidence threshold: {self.confidence_threshold}')
        self.get_logger().info(f'ğŸ“‹ Target classes: {self.target_classes if self.target_classes else "All classes"}')
        self.get_logger().info(f'ğŸ¤– Target objects: {self.target_objects}')
        self.get_logger().info(f'ğŸ”„ RViz markers: {self.enable_rviz_markers}')

def main(args=None):
    rclpy.init(args=args)
    
    try:
        node = YOLODetectorNode()
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print(f'Error: {e}')
    finally:
        if rclpy.ok():
            rclpy.shutdown()

if __name__ == '__main__':
    main()